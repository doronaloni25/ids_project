{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "66c9f39d",
      "metadata": {},
      "source": [
        "# How to run the Code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38d16cf2",
      "metadata": {},
      "source": [
        "1. Open the notebook in Google Colab. \n",
        "2. Set the runtime to GPU for faster execution\n",
        "3. Run the code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21860d26",
      "metadata": {},
      "source": [
        "# Code starts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85d36ad0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85d36ad0",
        "outputId": "57804276-d217-4abb-d0e8-c1dcd56b063e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install kagglehub kaggle torch numpy scikit-learn --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-1Ro3D97RiGb",
      "metadata": {
        "id": "-1Ro3D97RiGb"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pq0Cikb4RbO5",
      "metadata": {
        "id": "Pq0Cikb4RbO5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from pathlib import Path\n",
        "import kagglehub\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae40e0c7",
      "metadata": {
        "id": "ae40e0c7"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5da63e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5da63e0",
        "outputId": "f3b06a62-7882-4fb4-f1d8-cd0a84301e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/yianqaq/adfa-ld?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3.48M/3.48M [00:01<00:00, 2.92MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the data set is under: /root/.cache/kagglehub/datasets/yianqaq/adfa-ld/versions/1/ADFA-LD\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Where kagglehub unpacked\n",
        "root = Path(kagglehub.dataset_download(\"yianqaq/adfa-ld\"))\n",
        "\n",
        "# The “master” folder of the original release\n",
        "base = root / \"ADFA-LD\"\n",
        "train_dir  = base / \"Training_Data_Master\"\n",
        "val_dir    = base / \"Validation_Data_Master\"\n",
        "attack_dir = base / \"Attack_Data_Master\"\n",
        "print(\"the data set is under:\" , base)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UrP8OA6ERorM",
      "metadata": {
        "id": "UrP8OA6ERorM"
      },
      "source": [
        "# Get data statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B1_Il9-7RsqF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1_Il9-7RsqF",
        "outputId": "e225549f-cca9-40fe-ebc6-0c723e94f6d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence length statistics:\n",
            "number of normal sequances: 5205\n",
            "number of attacks sequances: 746\n",
            "≤  200:  2072 sequences (34.82%)\n",
            "≤  300:  2742 sequences (46.08%)\n",
            "≤  400:  4291 sequences (72.11%)\n",
            "≤  500:  4568 sequences (76.76%)\n",
            "≤  600:  4748 sequences (79.78%)\n",
            "≤  700:  4928 sequences (82.81%)\n",
            "≤  800:  5090 sequences (85.53%)\n",
            "≤  900:  5190 sequences (87.21%)\n"
          ]
        }
      ],
      "source": [
        "print(\"Sequence length statistics:\")\n",
        "# Collect all filenames and labels\n",
        "normals = list(train_dir.glob(\"*.txt\")) + list(val_dir.glob(\"*.txt\"))\n",
        "attacks = list(attack_dir.rglob(\"*.txt\"))\n",
        "print(f\"number of normal sequances: {len(normals)}\")\n",
        "print(f\"number of attacks sequances: {len(attacks)}\")\n",
        "all_paths  = normals + attacks\n",
        "all_labels = [0]*len(normals) + [1]*len(attacks)\n",
        "# Analyze syscall sequence lengths\n",
        "lengths = []\n",
        "for path in all_paths:\n",
        "    text = path.read_text().strip()\n",
        "    tokens = text.split()\n",
        "    lengths.append(len(tokens))\n",
        "\n",
        "# Calculate proportions\n",
        "thresholds = [200, 300, 400, 500, 600, 700, 800, 900]\n",
        "length_counts = {t: sum(l <= t for l in lengths) for t in thresholds}\n",
        "\n",
        "total = len(lengths)\n",
        "\n",
        "for t in thresholds:\n",
        "    percent = 100 * length_counts[t] / total\n",
        "    print(f\"≤ {t:4}: {length_counts[t]:5} sequences ({percent:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6072a6a",
      "metadata": {
        "id": "c6072a6a"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44aa1da2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44aa1da2",
        "outputId": "288c1e99-ad02-4a2d-d231-72413237ba6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▸ #train 3808, #val 952, #test 1191\n",
            "▸ vocab size = 173\n"
          ]
        }
      ],
      "source": [
        "# Collect all filenames and labels\n",
        "normals = list(train_dir.glob(\"*.txt\")) + list(val_dir.glob(\"*.txt\"))\n",
        "attacks = list(attack_dir.rglob(\"*.txt\"))\n",
        "\n",
        "all_paths  = normals + attacks\n",
        "all_labels = [0]*len(normals) + [1]*len(attacks)\n",
        "\n",
        "# First split: carve off TEST (20%)\n",
        "trainval_paths, test_paths, trainval_labels, test_labels = train_test_split(\n",
        "    all_paths, all_labels,\n",
        "    test_size=0.20,\n",
        "    stratify=all_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Second split: from remaining 80%, carve off VAL (20% of that → 16% of total)\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    trainval_paths, trainval_labels,\n",
        "    test_size=0.20,\n",
        "    stratify=trainval_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Build syscall→index vocabulary on the TRAIN set only - will be used for all,\n",
        "# Note! if in the test there is a system call which was not seen in training it will put it as 0 (padding)\n",
        "cnt = Counter()\n",
        "for path in train_paths:\n",
        "    text = path.read_text().strip()\n",
        "    tokens = text.split()               # splits on any whitespace\n",
        "    ints   = map(int, tokens)           # convert each token to int\n",
        "    cnt.update(ints)                    # count frequencies\n",
        "\n",
        "syscall2idx = {sys: i+1 for i, (sys, _) in enumerate(cnt.most_common())}\n",
        "vocab_size   = len(syscall2idx) + 1  # +1 for padding (idx 0)\n",
        "vocab_size = len(syscall2idx) + 1\n",
        "\n",
        "# Dataset wrapper to handle the data to torch transition\n",
        "class SyscallDataset(Dataset):\n",
        "    def __init__(self, paths, labels, max_len=400):\n",
        "        self.paths, self.labels, self.max_len = paths, labels, max_len\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # get system calls sequance for a spesific test\n",
        "        text   = self.paths[i].read_text().strip()\n",
        "        tokens = text.split()\n",
        "        # get the integer corresponding to the sys call\n",
        "        seq    = [ syscall2idx.get(int(syscall), 0)\n",
        "                   for syscall in tokens ][:self.max_len]\n",
        "        # returns 2 tensors: sequance and its label\n",
        "        return torch.tensor(seq, dtype=torch.long), torch.tensor(self.labels[i], dtype=torch.long)\n",
        "\n",
        "# creates a batch, returns 2 tensors sequances[batch_size, max_seq_len] and labels[batch_size] - pads with 0 if needed\n",
        "def collate_fn(batch):\n",
        "    seqs, labels = zip(*batch)\n",
        "    return pad_sequence(seqs, batch_first=True), torch.stack(labels)\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "train_ds   = SyscallDataset(train_paths, train_labels)\n",
        "val_ds     = SyscallDataset(val_paths,   val_labels)\n",
        "test_ds    = SyscallDataset(test_paths,  test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True,  collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_ds,   batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_ds,  batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"▸ #train {len(train_ds)}, #val {len(val_ds)}, #test {len(test_ds)}\")\n",
        "print(f\"▸ vocab size = {vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1d3b3c1",
      "metadata": {
        "id": "f1d3b3c1"
      },
      "source": [
        "# MODEL DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bac0cef",
      "metadata": {
        "id": "6bac0cef"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, bidirectional=False):\n",
        "        super().__init__()\n",
        "        # Embedding: maps each syscall index to a vector of size emb_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        #defining the LSTM encoder\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            # if enabled each token can see also the future giving it more context to where it is in the sequance\n",
        "            # also if enabled the LSTM have 2 separate hidden states for forward and backward and the results will be [B, hidden_dim × (2 or 1)], where we concate the 2 if enabled.\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        # Hidden fully-connected layer with sigmoid activation\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        # Output layer projecting to two classes (normal vs attack)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        # take output at last time step\n",
        "        last_hidden_state = lstm_out[:, -1, :]\n",
        "        hidden = self.sigmoid(self.fc1(last_hidden_state))\n",
        "        # use dropout to not overfit \n",
        "        hidden = self.dropout(hidden)\n",
        "        logits = self.fc2(hidden)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "913f23e9",
      "metadata": {
        "id": "913f23e9"
      },
      "source": [
        "# model, loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffc4ed17",
      "metadata": {
        "id": "ffc4ed17"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# defining the model here!\n",
        "model = LSTMClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    emb_dim=256,\n",
        "    hidden_dim=512,\n",
        "    num_layers=2,\n",
        "    bidirectional=False\n",
        ").to(device)\n",
        "\n",
        "# loss: binary cross-entropy with logits\n",
        "loss_fn  = nn.CrossEntropyLoss()\n",
        "# define adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "# to control the learning rate\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=2\n",
        ")\n",
        "# Class to stop the training and not overfit\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.01):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.should_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e52b0ccc",
      "metadata": {
        "id": "e52b0ccc"
      },
      "source": [
        "# TRAINING & VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d80d67cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d80d67cd",
        "outputId": "23253b15-4ec2-4422-f9a8-ec5fe9d62cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1 | train loss: 0.3650 | val loss:   0.3064 | val acc:    0.8855 | prec:       0.5521 | rec:        0.4454 | f1:         0.4930 |\n",
            "Epoch  2 | train loss: 0.3217 | val loss:   0.2976 | val acc:    0.9034 | prec:       0.7755 | rec:        0.3193 | f1:         0.4524 |\n",
            "Epoch  3 | train loss: 0.3232 | val loss:   0.3467 | val acc:    0.8834 | prec:       0.6538 | rec:        0.1429 | f1:         0.2345 |\n",
            "Epoch  4 | train loss: 0.3363 | val loss:   0.2768 | val acc:    0.8887 | prec:       0.5586 | rec:        0.5210 | f1:         0.5391 |\n",
            "Epoch  5 | train loss: 0.2767 | val loss:   0.2387 | val acc:    0.8992 | prec:       0.6292 | rec:        0.4706 | f1:         0.5385 |\n",
            "Epoch  6 | train loss: 0.2243 | val loss:   0.1975 | val acc:    0.9139 | prec:       0.6259 | rec:        0.7731 | f1:         0.6917 |\n",
            "Epoch  7 | train loss: 0.1734 | val loss:   0.1834 | val acc:    0.9254 | prec:       0.6558 | rec:        0.8487 | f1:         0.7399 |\n",
            "Epoch  8 | train loss: 0.1548 | val loss:   0.1393 | val acc:    0.9496 | prec:       0.7886 | rec:        0.8151 | f1:         0.8017 |\n",
            "Epoch  9 | train loss: 0.1413 | val loss:   0.1408 | val acc:    0.9412 | prec:       0.7838 | rec:        0.7311 | f1:         0.7565 |\n",
            "Epoch 10 | train loss: 0.1309 | val loss:   0.1553 | val acc:    0.9359 | prec:       0.7266 | rec:        0.7815 | f1:         0.7530 |\n",
            "Epoch 11 | train loss: 0.1760 | val loss:   0.2086 | val acc:    0.9086 | prec:       0.7286 | rec:        0.4286 | f1:         0.5397 |\n",
            "Epoch 12 | train loss: 0.1856 | val loss:   0.1750 | val acc:    0.9338 | prec:       0.7593 | rec:        0.6891 | f1:         0.7225 |\n",
            "Epoch 13 | train loss: 0.1246 | val loss:   0.1361 | val acc:    0.9412 | prec:       0.7944 | rec:        0.7143 | f1:         0.7522 |\n",
            "Epoch 14 | train loss: 0.1038 | val loss:   0.1341 | val acc:    0.9433 | prec:       0.7826 | rec:        0.7563 | f1:         0.7692 |\n",
            "Epoch 15 | train loss: 0.0984 | val loss:   0.1183 | val acc:    0.9527 | prec:       0.8304 | rec:        0.7815 | f1:         0.8052 |\n",
            "Epoch 16 | train loss: 0.0962 | val loss:   0.1219 | val acc:    0.9580 | prec:       0.7970 | rec:        0.8908 | f1:         0.8413 |\n",
            "Epoch 17 | train loss: 0.0864 | val loss:   0.1176 | val acc:    0.9569 | prec:       0.7868 | rec:        0.8992 | f1:         0.8392 |\n",
            "Epoch 18 | train loss: 0.0704 | val loss:   0.1074 | val acc:    0.9632 | prec:       0.8231 | rec:        0.8992 | f1:         0.8594 |\n",
            "Epoch 19 | train loss: 0.0663 | val loss:   0.1059 | val acc:    0.9601 | prec:       0.8462 | rec:        0.8319 | f1:         0.8390 |\n",
            "Epoch 20 | train loss: 0.0674 | val loss:   0.1175 | val acc:    0.9569 | prec:       0.8750 | rec:        0.7647 | f1:         0.8161 |\n",
            "Epoch 21 | train loss: 0.0639 | val loss:   0.1138 | val acc:    0.9559 | prec:       0.8889 | rec:        0.7395 | f1:         0.8073 |\n",
            "Epoch 22 | train loss: 0.0550 | val loss:   0.1143 | val acc:    0.9611 | prec:       0.8534 | rec:        0.8319 | f1:         0.8426 |\n",
            "Epoch 23 | train loss: 0.0427 | val loss:   0.1113 | val acc:    0.9632 | prec:       0.8621 | rec:        0.8403 | f1:         0.8511 |\n",
            "Early stopping triggered.\n"
          ]
        }
      ],
      "source": [
        "def train_epoch(loader):\n",
        "    # Puts the model into “training” mode (enables dropout, batch-norm updates and more)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    # Iterates over your DataLoader, where each seqs is a tensor of shape [B, seq_len] and labels is [B].\n",
        "    for seqs, labels in loader:\n",
        "        seqs, labels = seqs.to(device), labels.to(device)\n",
        "        # forward pass\n",
        "        logits = model(seqs)\n",
        "        # compute loss\n",
        "        loss   = loss_fn(logits, labels)\n",
        "        # backpropogation: Zeroes old gradients, computes new ones, and takes an optimizer step.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # loss.item() is the average loss over the batch; multiplying by B gives the total for that batch.\n",
        "        total_loss += loss.item() * seqs.size(0)\n",
        "    # returns the avarage training loss\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_epoch(loader, isTest=False):\n",
        "    # Switches to evaluation mode (disables dropout, freezes batch-norm).\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct    = 0\n",
        "    preds_list = []\n",
        "    labels_list = []\n",
        "    with torch.no_grad():\n",
        "        for seqs, labels in loader:\n",
        "            seqs, labels = seqs.to(device), labels.to(device)\n",
        "            logits = model(seqs)\n",
        "            loss   = loss_fn(logits, labels)\n",
        "            # loss.item() is the average loss over the batch; multiplying by B gives the total for that batch.\n",
        "            total_loss += loss.item() * seqs.size(0)\n",
        "            # until here same as training, now get the prediction by argmax on the probability vectors (normal, attack)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            # Move predictions and labels to CPU and add to lists\n",
        "            preds_list.extend(preds.cpu().tolist())\n",
        "            labels_list.extend(labels.cpu().tolist())\n",
        "\n",
        "    # After all batches, compute overall metrics:\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    # Accuracy: fraction of correct predictions\n",
        "    accuracy = (torch.tensor(preds_list) == torch.tensor(labels_list)).float().mean().item()\n",
        "    # Precision, recall, F1 for the positive (attack) class\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels_list, preds_list, average='binary', zero_division=0)\n",
        "    if(isTest):\n",
        "        tn, fp, fn, tp = confusion_matrix(labels_list, preds_list).ravel()\n",
        "        print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
        "    return avg_loss, accuracy, precision, recall, f1\n",
        "\n",
        "num_epochs = 30\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
        "for epoch in range(1, num_epochs+1):\n",
        "        train_loss = train_epoch(train_loader)\n",
        "        val_loss, val_acc, val_prec, val_rec, val_f1 = eval_epoch(val_loader)\n",
        "        scheduler.step(val_loss)\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.should_stop:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "        print(\n",
        "            f\"Epoch {epoch:2d} |\"\n",
        "            f\" train loss: {train_loss:.4f} |\"\n",
        "            f\" val loss:   {val_loss:.4f} |\"\n",
        "            f\" val acc:    {val_acc:.4f} |\"\n",
        "            f\" prec:       {val_prec:.4f} |\"\n",
        "            f\" rec:        {val_rec:.4f} |\"\n",
        "            f\" f1:         {val_f1:.4f} |\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd202bb4",
      "metadata": {
        "id": "dd202bb4"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289a3079",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "289a3079",
        "outputId": "06876673-65bc-4fd1-d771-4b4673cb51b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TP: 131, FP: 19, TN: 1023, FN: 18\n",
            " loss: 0.1154 | acc:    0.9653 | prec:       0.8772 | rec:        0.8403 | f1:         0.8584 |\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc, test_prec, test_rec, test_f1 = eval_epoch(test_loader, True)\n",
        "print(\n",
        "            f\" loss: {test_loss:.4f} |\"\n",
        "            f\" acc:    {val_acc:.4f} |\"\n",
        "            f\" prec:       {val_prec:.4f} |\"\n",
        "            f\" rec:        {val_rec:.4f} |\"\n",
        "            f\" f1:         {val_f1:.4f} |\"\n",
        "        )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
